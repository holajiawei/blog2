---
title: FAST11最佳论文《A Study of Practical Deduplication》
description: 
published: true
date: 2021-01-24T08:32:23.073Z
tags: deduplication, microsoft
editor: markdown
dateCreated: 2021-01-24T08:32:23.073Z
---

# FAST11最佳论文《A Study of Practical Deduplication》
> 论文非常简单，从名称上也能看出来，是一项实践的总结。论文地址<https://www.usenix.org/legacy/events/fast11/tech/full_papers/Meyer.pdf>

## 内容分析

作者通过在微软内部召集了1000余位测试者进行了一项横跨857份文件系统、持续一个月的统计测试，测试了以下的内容:
- deduplication算法的比较：基于固定块 和基于内容定义的可变快(以Rabin为例)的不同去重算法的比较
- 块大小对deduplication效果的影响: 从8K到64K的不同窗口大小的比较
- 去重范围的比较: 所有文件和基于部分文件的去重效率的比较
- 不同文件格式对去重效率的影响
- 用户使用文件习惯的统计


## 实施过程

### 设计VSS(Volume Shadow Copy Service)

### 2-Pass
作者由于只需要统计相同的那部分内容，反而对于唯一的内容不感兴趣，所以设计了一个两步过滤的算法。
准备两个同样大小的布隆过滤器2GB，扫描日志中的每个哈希条目，并插入到第一个布隆过滤器中，如果第一个布隆过滤器已存在相同的条目，就将其插入到第二个布隆过滤器中。然后进行第二遍扫描，只将条目与第二个布隆过滤器，如果第二个布隆过滤器找不多熬，就可以确定该hash条目仅被看到1次，就可以被忽略了。当然了布隆过滤器是有一定误判的概率的，作者在后续的处理方案中将这种特殊情况考虑到了，最后是在数据库筛选时清除掉的


## 结论
- 基于内容定义的可变块的去重算法要优于基于固定块的
- 基于8K的块大小去重效率要高于64K的，但这个影响远远小于不同算法带来的影响
- 基于完整所有文件的去重效率要高于基于部分文件的
- 不同文件格式的去重效率不同
- 去重+ 压缩能更好优化存储空间
- 虽然存储空间整体随着时间不断变大，但用户的使用习惯相较于之前没有发生太多的变化
	- 60%的文件会在创建6个月内被修改，90%的文件会在创建36个月内修改
  - 文件从数量上来统计超过50%文件小于4M
  - 创建的文件夹95%概率不会超过5个文件夹
  - 创建的文件夹层次深度没有超过20层的，并且50%的是6层
  - 90%的文件夹内的文件数量小于10个